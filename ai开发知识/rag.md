# rag是什么
**rag 检索增强生成 基于用户的输入从外部知识数据检索与查询相关的文本片段，然后作为上下文发送向ai提问**
# 工作流程
* 索引的创建 （**数据的收集和存储**）
1. 从不同的数据源收集原始文档
2. 清洗这些文档为一个标准的文档格式
3. 把标准化的长文档切割为适当大小的片段（**chunks**）
4. 使用Embedding 模型把文档切片转化为高维向量
5. 把相关的高维向量存储在向量数据库中
* 检索生成
1. 将用户的提问 清洗为一个标准的文档格式
2. 使用 Embedding 模型 转化为相关的向量
3. 查询向量数据库获取相关的向量数据
4. 将获取向量数据的信息组装为输入LLM的上下文
# 检索生成 的优化
## 召回
在查询向量数据库获取相关的向量数据 这个步骤中 向量数据库强调**速度和广度而非精确度**，只会尽可能的收集相关的数据集数量
## 粗排
对召回的数据进行初步的筛选，把返回的数量减少，提高返回数据集的质量
## 精排 和 Rank模型
* 对粗排的内容进行更加精确的排序，进一步提高返回数据集的质量 (及是选出相关的Top-k)
* 消除文档之间的细微语义关系
* 所用的索引的速度较慢，但是精度较好
* 交叉编码器模型 如 BERT
**交叉编码器
- **工作原理**：
    1. 将两个需要比较的文本（例如，一个查询和一个文档，一个前提和一个假设，两个句子）**拼接在一起**，中间用特殊分隔符（如 `[SEP]`）隔开。
        
    2. 将这个拼接后的长序列输入到一个**单一的深度 Transformer 模型**（如 BERT、RoBERTa）中。
        
    3. 模型会同时处理这两个文本，并进行**深度、全面的交互和双向注意力计算**。这意味着在编码过程中，第一个文本的每一个词都能关注到第二个文本的所有词，反之亦然。
        
    4. 最终，模型输出一个**单一的表示**（通常取 `[CLS]` 标记的嵌入），并通过一个分类层来预测这两个文本之间的关系（如相关性分数、蕴含关系、相似度等）
## 混合检索
混合检索的核心思想是 **“取长补短”**
1. **传统关键词检索**
    
    - **原理**：基于词频、逆向文档频率等统计信息，精确匹配查询词和文档中的词汇。
        
    - **代表技术**：BM25（被认为是传统关键词检索的“黄金标准”）。
        
    - **优点**：
        
        - 精确匹配，对于事实性、术语性查询效果好。
            
        - 计算效率高，可解释性强。
            
        - 不需要训练数据。
            
    - **缺点**：
        
        - 受限于词汇表面形式，无法处理同义词、简写、语义相关但不包含相同关键词的情况（“词汇鸿沟”问题）。
            
        - 例如，搜索“猫”，可能无法返回包含“feline”或“kitty”但没写“猫”的文档。
            
2. **向量/语义检索**
    
    - **原理**：使用深度学习模型（如BERT、Sentence Transformers）将查询和文档都转换为高维向量（嵌入）。通过计算向量间的相似度（如余弦相似度）来检索。
        
    - **优点**：
        
        - 能理解查询和文档的**语义**。即使词汇不匹配，只要意思相近就能找到。
            
        - 能处理复杂的、基于含义的查询。
            
        - 例如，搜索“宠物猫的日常护理”，可以找到关于“如何照顾家养猫咪”的文档。
            
    - **缺点**：
        
        - 可能忽略重要的关键词匹配。
            
        - 对于特定术语、名称、ID等精确匹配效果可能不如关键词检索。
            
        - 计算成本相对较高。
            
        - 需要合适的模型和嵌入生成。